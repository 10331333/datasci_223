{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Risk factors for Coronary Heart Disease \n",
    "\n",
    "Dataset downloaded from Kaggle example [Behavioral Risk Factor Surveillance System](https://www.kaggle.com/datasets/cdc/behavioral-risk-factor-surveillance-system/data)\n",
    "\n",
    " - Load libraries\n",
    " - Get data from Kaggle\n",
    " - Data cleaning\n",
    "    - Variable selection \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy (from -r requirements.txt (line 1))\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting pandas (from -r requirements.txt (line 2))\n",
      "  Using cached pandas-2.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting matplotlib (from -r requirements.txt (line 3))\n",
      "  Using cached matplotlib-3.8.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow (from -r requirements.txt (line 4))\n",
      "  Using cached tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting imblearn (from -r requirements.txt (line 5))\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting ydata-profiling (from -r requirements.txt (line 6))\n",
      "  Using cached ydata_profiling-4.7.0-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Collecting torch (from -r requirements.txt (line 9))\n",
      "  Using cached torch-2.2.1-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Daphne/Library/Python/3.11/lib/python/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 3))\n",
      "  Using cached contourpy-1.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 3))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 3))\n",
      "  Using cached fonttools-4.50.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (159 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Daphne/Library/Python/3.11/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (23.2)\n",
      "Collecting pillow>=8 (from matplotlib->-r requirements.txt (line 3))\n",
      "  Using cached pillow-10.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (24.3.7)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (18.1.1)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (69.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/Daphne/Library/Python/3.11/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached keras-3.1.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting imbalanced-learn (from imblearn->-r requirements.txt (line 5))\n",
      "  Using cached imbalanced_learn-0.12.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting scipy<1.12,>=1.4.1 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached scipy-1.11.4-cp311-cp311-macosx_12_0_arm64.whl.metadata (165 kB)\n",
      "Collecting pydantic>=2 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting PyYAML<6.1,>=5.0.0 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting jinja2<3.2,>=2.11.1 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting visions<0.7.7,>=0.7.5 (from visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached visions-0.7.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in /opt/homebrew/lib/python3.11/site-packages (from ydata-profiling->-r requirements.txt (line 6)) (0.1.12)\n",
      "Collecting phik<0.13,>=0.11.1 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached phik-0.12.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in /opt/homebrew/lib/python3.11/site-packages (from ydata-profiling->-r requirements.txt (line 6)) (4.66.2)\n",
      "Collecting seaborn<0.13,>=0.10.1 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting multimethod<2,>=1.4 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached multimethod-1.11.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting statsmodels<1,>=0.13.2 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached statsmodels-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Collecting typeguard<5,>=4.1.2 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached typeguard-4.1.5-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting imagehash==4.3.1 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting wordcloud>=1.9.1 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached wordcloud-1.9.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting dacite>=1.8 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached dacite-1.8.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting numba<1,>=0.56.0 (from ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached numba-0.59.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting PyWavelets (from imagehash==4.3.1->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached pywavelets-1.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting filelock (from torch->-r requirements.txt (line 9))\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sympy (from torch->-r requirements.txt (line 9))\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->-r requirements.txt (line 9))\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting fsspec (from torch->-r requirements.txt (line 9))\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 4)) (0.42.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2<3.2,>=2.11.1->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: namex in /opt/homebrew/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow->-r requirements.txt (line 4)) (0.0.7)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached optree-0.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (45 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba<1,>=0.56.0->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached llvmlite-0.42.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting joblib>=0.14.1 (from phik<0.13,>=0.11.1->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic>=2->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached pydantic_core-2.16.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 4)) (2.2.1)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting patsy>=0.5.4 (from statsmodels<1,>=0.13.2->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting attrs>=19.3.0 (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling->-r requirements.txt (line 6))\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting scikit-learn>=1.0.2 (from imbalanced-learn->imblearn->-r requirements.txt (line 5))\n",
      "  Using cached scikit_learn-1.4.1.post1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from imbalanced-learn->imblearn->-r requirements.txt (line 5))\n",
      "  Using cached threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch->-r requirements.txt (line 9)) (1.3.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/Daphne/Library/Python/3.11/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow->-r requirements.txt (line 4)) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow->-r requirements.txt (line 4))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached pandas-2.2.1-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached matplotlib-3.8.3-cp311-cp311-macosx_11_0_arm64.whl (7.5 MB)\n",
      "Using cached tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl (227.0 MB)\n",
      "Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached ydata_profiling-4.7.0-py2.py3-none-any.whl (357 kB)\n",
      "Using cached ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "Using cached torch-2.2.1-cp311-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached contourpy-1.2.0-cp311-cp311-macosx_11_0_arm64.whl (243 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached dacite-1.8.1-py3-none-any.whl (14 kB)\n",
      "Using cached fonttools-4.50.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
      "Using cached h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached keras-3.1.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl (66 kB)\n",
      "Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "Using cached multimethod-1.11.2-py3-none-any.whl (10 kB)\n",
      "Using cached numba-0.59.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached phik-0.12.4-cp311-cp311-macosx_11_0_arm64.whl (657 kB)\n",
      "Using cached pillow-10.2.0-cp311-cp311-macosx_11_0_arm64.whl (3.3 MB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "Using cached pydantic_core-2.16.3-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached scipy-1.11.4-cp311-cp311-macosx_12_0_arm64.whl (29.7 MB)\n",
      "Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Using cached statsmodels-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (10.1 MB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl (3.4 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
      "Using cached visions-0.7.6-py3-none-any.whl (104 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached wordcloud-1.9.3-cp311-cp311-macosx_11_0_arm64.whl (168 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached imbalanced_learn-0.12.0-py3-none-any.whl (257 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached llvmlite-0.42.0-cp311-cp311-macosx_11_0_arm64.whl (28.8 MB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached scikit_learn-1.4.1.post1-cp311-cp311-macosx_12_0_arm64.whl (10.4 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached optree-0.10.0-cp311-cp311-macosx_11_0_arm64.whl (250 kB)\n",
      "Using cached pywavelets-1.5.0-cp311-cp311-macosx_11_0_arm64.whl (4.3 MB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: typeguard, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, sympy, PyYAML, pyparsing, pydantic-core, protobuf, pillow, optree, numpy, networkx, multimethod, mdurl, MarkupSafe, markdown, llvmlite, kiwisolver, joblib, idna, grpcio, google-pasta, gast, fsspec, fonttools, filelock, dacite, cycler, charset-normalizer, certifi, attrs, astunparse, annotated-types, absl-py, werkzeug, scipy, requests, PyWavelets, pydantic, patsy, pandas, opt-einsum, numba, ml-dtypes, markdown-it-py, jinja2, h5py, contourpy, visions, torch, tensorboard, statsmodels, scikit-learn, rich, matplotlib, imagehash, wordcloud, seaborn, phik, keras, imbalanced-learn, ydata-profiling, tensorflow, imblearn\n",
      "Successfully installed MarkupSafe-2.1.5 PyWavelets-1.5.0 PyYAML-6.0.1 absl-py-2.1.0 annotated-types-0.6.0 astunparse-1.6.3 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 contourpy-1.2.0 cycler-0.12.1 dacite-1.8.1 filelock-3.13.1 fonttools-4.50.0 fsspec-2024.3.1 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 idna-3.6 imagehash-4.3.1 imbalanced-learn-0.12.0 imblearn-0.0 jinja2-3.1.3 joblib-1.3.2 keras-3.1.0 kiwisolver-1.4.5 llvmlite-0.42.0 markdown-3.6 markdown-it-py-3.0.0 matplotlib-3.8.3 mdurl-0.1.2 ml-dtypes-0.3.2 multimethod-1.11.2 networkx-3.2.1 numba-0.59.0 numpy-1.26.4 opt-einsum-3.3.0 optree-0.10.0 pandas-2.2.1 patsy-0.5.6 phik-0.12.4 pillow-10.2.0 protobuf-4.25.3 pydantic-2.6.4 pydantic-core-2.16.3 pyparsing-3.1.2 requests-2.31.0 rich-13.7.1 scikit-learn-1.4.1.post1 scipy-1.11.4 seaborn-0.12.2 statsmodels-0.14.1 sympy-1.12 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 threadpoolctl-3.3.0 torch-2.2.1 typeguard-4.1.5 visions-0.7.6 werkzeug-3.0.1 wordcloud-1.9.3 ydata-profiling-4.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlalchemy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mydata_profiling\u001b[39;00m \u001b[39mimport\u001b[39;00m ProfileReport\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msqlalchemy\u001b[39;00m \u001b[39mimport\u001b[39;00m create_engine\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sqlalchemy'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imblearn\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data\n",
    "with zipfile.ZipFile('BRFSS.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each CSV into a DataFrame\n",
    "tables = []\n",
    "for f in os.listdir('data'):\n",
    "    if f.endswith('.csv'):\n",
    "        table_name = 'df'+f.replace('.csv', '')\n",
    "        file_path = os.path.join('data', f)\n",
    "        # tables[table_name] = pd.read_csv(file_path)\n",
    "        tables.append(table_name)\n",
    "        globals()[table_name] = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load json mapping file into a dictionary\n",
    "import json\n",
    "with open('data/2015_formats.json', 'r') as file: \n",
    "    mapping = json.load(file)\n",
    "# Example: codebook for 'CVD4CRHD' ('CVDCRHD4' in .csv file)\n",
    "    # Note: variable codes in codebook and .csv files may not be the same\n",
    "mapping['CVD4CRHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "(of BRFSS 2015 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {df2015.shape[0]} rows and {df2015.shape[1]} columns in the original data set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select target and predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to store variables and their corresponding BRFSS code \n",
    "varname_code_mapping = {\n",
    "    # target variable\n",
    "    'chd': 'CVDCRHD4', \n",
    "    # predictor variables\n",
    "        #demographic variables\n",
    "    'sex': 'SEX', \n",
    "    'race': '_RACE_G1',\n",
    "    'age5y': '_AGEG5YR',  \n",
    "    'ever_married': 'MARITAL',\n",
    "        #socioeconomic status variables\n",
    "    'education':'_EDUCAG', \n",
    "    'income': '_INCOMG',\n",
    "    'employment': 'EMPLOY1', \n",
    "        # health variables\n",
    "    'diabetes': 'DIABETE3', \n",
    "    'hypertension': 'BPHIGH4', \n",
    "    'bmi': '_BMI5',\n",
    "        # behavioral variables\n",
    "    'smoke': '_SMOKER3',\n",
    "    'vegetable': '_VEGLT1',\n",
    "    'fruit': '_FRTLT1',\n",
    "    'binge_drink': 'DRNK3GE5',\n",
    "    'exercise': 'EXERANY2', \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take data from 2015 BFRSS data frame\n",
    "df = df2015\n",
    "\n",
    "# Record numerical variables\n",
    "numerical_variables = ['bmi', 'binge_drink']\n",
    "\n",
    "# Select variables and store in new data frame\n",
    "    # Catagorical variables are stored as strings\n",
    "    # Numerical variables are transformed as needed when imported\n",
    "dirty_df = pd.DataFrame()\n",
    "for key, code in varname_code_mapping.items(): \n",
    "    if key not in numerical_variables: \n",
    "        dirty_df[key] = df[code]\n",
    "    elif key == 'bmi':\n",
    "        # Return true values of bmi (divide by 100)\n",
    "        dirty_df[key] = df[code]/100\n",
    "    elif key == 'binge_drink': \n",
    "        dirty_df[key] = df[code].replace({88: 0, 77: np.nan, 99: np.nan})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {dirty_df.shape[0]} rows and {dirty_df.shape[1]} columns in the selected data set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overview of numerical variables\n",
    "dirty_df[numerical_variables].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "\n",
    "#### - Remove unrealistic data (BMI>= 18.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI is unlikely to be lower than 18.5\n",
    "dirty_df = dirty_df[dirty_df['bmi'] >= 18.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Recode variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique codes of binary variables and print out \n",
    "# Collect binary variables need to be recoded \n",
    "binary_var = ['chd', 'sex', 'ever_married', 'diabetes', 'hypertension', 'vegetable', 'fruit', 'exercise']\n",
    "recode_binary_var = []\n",
    "for var in binary_var: \n",
    "    print(var)\n",
    "    print(dirty_df[var].unique())\n",
    "    if len(dirty_df[var].unique())>2: \n",
    "        recode_binary_var.append(var)\n",
    "print(f\"Binary variables that need to be recoded: {recode_binary_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Recode instructions \n",
    "Codes for 'hypertension' and 'diabetes'\n",
    " '1': \"Yes\"\n",
    " '2': '\"Yes, but female told only during pregnancy\"' -> np.nan\n",
    " '3': '\"No\"' -> '0'\n",
    " '4': '\"Told borderline high or pre-condition\"' -> np.nan\n",
    " '7': '\"Don't know/Not Sure\"' -> np.nan\n",
    " '9': '\"Refused\"' --> np.nan\n",
    "\"\"\"\n",
    "# Recode variables ['hypertension', 'diabetes'] into binary variables\n",
    "vars = ['hypertension', 'diabetes']\n",
    "for col in vars: \n",
    "    dirty_df[col]= dirty_df[col].replace({2.: np.nan, 3.: 0,  4.: np.nan, \n",
    "                                          7.: np.nan, 9.: np.nan})\n",
    "\n",
    "\"\"\"\n",
    "Codes for 'vegetable' and 'fruit'\n",
    " '1': '\"Consumed one or more times per day\"' \n",
    " '2': '\"Consumed less than one time per day\"' -> 0\n",
    " '9': '\"DonÂ´t know, refused or missing values\" -> np.nan\n",
    "\n",
    "Codes for 'exercise' and 'chd'\n",
    " '1': '\"Yes\"'\n",
    " '2': '\"No\"' -> 0\n",
    " '7': '\"Don't know/Not sure\"'-> np.nan\n",
    " '9': '\"Refused\"' -> np.nan\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Recode variables ['vegetable', 'fruit', 'exercise'] into binary variables\n",
    "vars = ['chd', 'vegetable', 'fruit', 'exercise']\n",
    "for col in vars: \n",
    "    dirty_df[col]= dirty_df[col].replace({2.: 0, 7.: np.nan, 9.: np.nan})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Codes for 'ever_married'\n",
    " '1': '\"Married\"' -> 1\n",
    " '2': '\"Divorced\"' -> 1\n",
    " '3': '\"Widowed\"' -> 1\n",
    " '4': '\"Separated\"' -> 1\n",
    " '5': '\"Never married\"' -> 0\n",
    " '6': '\"A member of an unmarried couple\"' -> 0\n",
    " '9': '\"Refused\"' -> np.nan\n",
    "\"\"\"\n",
    "\n",
    "# Recode 'ever_married' as binary variable (0 = No, 1 = Yes)\n",
    "\n",
    "dirty_df['ever_married'] = dirty_df['ever_married'].replace({1.: 1, 2.: 1, 3.: 1, 4.: 1, \n",
    "                                                             5.: 0, 6.: 0, \n",
    "                                                             9.: np.nan})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Codes for 'age5y'\n",
    " '1': '\"Age 18 to 24\"',\n",
    " '2': '\"Age 25 to 29\"',\n",
    " '3': '\"Age 30 to 34\"',\n",
    " '4': '\"Age 35 to 39\"',\n",
    " '5': '\"Age 40 to 44\"',\n",
    " '6': '\"Age 45 to 49\"',\n",
    " '7': '\"Age 50 to 54\"',\n",
    " '8': '\"Age 55 to 59\"',\n",
    " '9': '\"Age 60 to 64\"',\n",
    " '10': '\"Age 65 to 69\"',\n",
    " '11': '\"Age 70 to 74\"',\n",
    " '12': '\"Age 75 to 79\"',\n",
    " '13': '\"Age 80 or older\"',\n",
    " '14': '\"Don't know/Refused/Missing\" -> np.nan\n",
    "\"\"\"\n",
    "dirty_df['age5y'] = dirty_df['age5y'].replace({14.: np.nan})\n",
    "\n",
    "\"\"\"\n",
    "Codes for 'education'\n",
    " '1': '\"Did not graduate High School\"',\n",
    " '2': '\"Graduated High School\"',\n",
    " '3': '\"Attended College or Technical School\"',\n",
    " '4': '\"Graduated from College or Technical School\"',\n",
    " '9': '\"Don't know/Not sure/Missing\"-> np.nan\n",
    "\n",
    "Codes for 'income'\n",
    " '1': '\"Less than $15,000\"',\n",
    " '2': '\"$15,000 to less than $25,000\"',\n",
    " '3': '\"$25,000 to less than $35,000\"',\n",
    " '4': '\"$35,000 to less than $50,000\"',\n",
    " '5': '\"$50,000 or more\"',\n",
    " '9': '\"Don't know/Not sure/Missing\"'-> np.nan\n",
    "\"\"\"\n",
    "vars = ['education', 'income']\n",
    "for col in vars: \n",
    "    dirty_df[col]= dirty_df[col].replace({9.: np.nan})\n",
    "\n",
    "\"\"\"\" \n",
    "Codes for 'employment':\n",
    " '1': '\"Employed for wages\"'\n",
    " '2': '\"Self-employed\"'\n",
    " '3': '\"Out of work for 1 year or more\"'\n",
    " '4': '\"Out of work for less than 1 year\"'\n",
    " '5': '\"A homemaker\"'\n",
    " '6': '\"A student\"'\n",
    " '7': '\"Retired\"'\n",
    " '8': '\"Unable to work\"' -> np.nan (exclude since those unable to work)\n",
    " '9': '\"Refused\"' -> np.nan\n",
    "\n",
    " Note: Those unable to work are excluded since there is a high possibility that they may have other health conditions. \n",
    "\"\"\"\n",
    "\n",
    "dirty_df['employment'] = dirty_df['employment'].replace({8.:np.nan, 9.: np.nan})\n",
    "\n",
    "\"\"\"\n",
    "Codes for 'smoke':\n",
    " '1': '\"Current smoker - now smokes every day\"',\n",
    " '2': '\"Current smoker - now smokes some days\"' --> '1'\n",
    " '3': '\"Former smoker\"', -> '2'\n",
    " '4': '\"Never smoked\"', --> '0'\n",
    " '9': '\"Don't know/Refused/Missing\"' -> np.nan\n",
    "\"\"\"\n",
    "# Recode 'smoke' into 3 catagories (1 = current smoker, 2 = never smoked, 3 = former smoker)\n",
    "\n",
    "dirty_df['smoke'] = dirty_df['smoke'].replace({2.:1, 3.: 2, 4.: 0, 9.: np.nan})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Verify that variables are correctly recoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all levels are correctly assigned to new levels\n",
    "\n",
    "for key, code in varname_code_mapping.items(): \n",
    "    if key not in numerical_variables: \n",
    "        print(pd.crosstab(dirty_df[key],df2015[code]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data \n",
    "clean_df = dirty_df.dropna()\n",
    "clean_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_count= clean_df.chd.value_counts()\n",
    "target_count.plot(kind='bar', title='Count (target)')\n",
    "for i, count in enumerate(target_count):\n",
    "    plt.annotate(str(count), xy=(i, count), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('cleaned_data_imbalanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ydata-profiling report of dirty data file\n",
    "profile = ProfileReport(clean_df, title = \"Profile Report\")\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Random downsampling\n",
    "\n",
    "Imbalance of data is observed in the between those diagnosed with angina or coronary heart disease (`chd`=1) and those who were not (`chd`=0). Random sampling was done to balance the number of samples in two groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = clean_df.chd.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_chd0 = clean_df[clean_df['chd'] == 0.]\n",
    "df_chd1 = clean_df[clean_df['chd'] == 1.]\n",
    "\n",
    "# Under sampling\n",
    "df_chd0_under = df_chd0.sample(count_class_1)\n",
    "cleandf_undersample = pd.concat([df_chd0_under, df_chd1], axis=0)\n",
    "\n",
    "cleandf_undersample.chd.value_counts().plot(kind='bar', title='Count (target)')\n",
    "for i, count in enumerate(cleandf_undersample.chd.value_counts()):\n",
    "    plt.annotate(str(count), xy=(i, count), ha='center', va='bottom')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in varname_code_mapping.keys(): \n",
    "    if var not in numerical_variables:\n",
    "        print(pd.crosstab(cleandf_undersample['chd'],cleandf_undersample[var]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ydata-profiling report of clean & downsampled data file\n",
    "profile = ProfileReport(cleandf_undersample, title = \"Profile Report\")\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf_undersample.to_csv('cleaned_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
