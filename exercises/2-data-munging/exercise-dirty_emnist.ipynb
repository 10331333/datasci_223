{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning, so we can build a classifiier\n",
    "\n",
    "### **Note:** Remember to check which environment this notebook is using!\n",
    "\n",
    "## Steps in this notebook\n",
    "1. Get the packages we need (install -> import)\n",
    "2. `emnist` pulls a clean dataset of handwritten characters\n",
    "    1. Clean dataframes are `raw_train` and `raw_test`\n",
    "    2. Each has only two columns\n",
    "        1. 'image' - 28x28 pixel array images of handwritten characters\n",
    "        2. 'label' - different number for class of digit: 0 -> 0, 10 -> A, 61 -> z\n",
    "3. Demonstrate basic plotting of the images\n",
    "4. Intentionally dirty this clean dataset\n",
    "    1. Dirty dataframes are `dirty_train` and `dirty_test`\n",
    "    2. Look through how the dirtying process works for ideas on how to clean up\n",
    "5. Space for whatever you're going to do next\n",
    "    1. Get cleaning with pandas\n",
    "    2. Build classifiers with the data you've cleaned\n",
    "    3. Skip data cleaning and build classifier with the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages\n",
    "\n",
    "Uncomment the followinig lines the first time running this notebook in this environment\n",
    "`%conda install pandas numpy emnist matplotlib`\n",
    "`%pip install emnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emnist\n",
    "from hashlib import sha1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, and reshape it into a 28x28 array\n",
    "\n",
    "# The size of each image is 28x28 pixels\n",
    "size = 28 \n",
    "\n",
    "# Extract the training split as images and labels\n",
    "image, label = emnist.extract_training_samples('byclass')\n",
    "\n",
    "# Add columns for each pixel value (28x28 = 784 columns)\n",
    "raw_train = pd.DataFrame()\n",
    "\n",
    "# Add a column showing the label\n",
    "raw_train['label'] = label\n",
    "\n",
    "# Add a column with the image data as a 28x28 array\n",
    "raw_train['image'] = list(image)\n",
    "\n",
    "\n",
    "# Repeat for the test split\n",
    "image, label = emnist.extract_test_samples('byclass')\n",
    "raw_test = pd.DataFrame()\n",
    "raw_test['label'] = label\n",
    "raw_test['image'] = list(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot individual images using matplotlib\n",
    "plt.imshow(raw_train['image'][0], cmap='gray')\n",
    "plt.show() # Show the plot (optional with a single image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row for each label\n",
    "firsts = raw_train.groupby('label').first().reset_index()\n",
    "\n",
    "# Build a plot with the first image for each label\n",
    "fig, ax = plt.subplots(7, 10, figsize=(10, 7))\n",
    "for i in range(62):\n",
    "    ax[i//10, i%10].imshow(firsts['image'][i], cmap='gray')\n",
    "    ax[i//10, i%10].axis('off')\n",
    "    ax[i//10, i%10].set_title(firsts['label'][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting dirty\n",
    "\n",
    "Now that we've got the data in good shape, let's rough it up a little.\n",
    "\n",
    "- [x] Add `predicit` col with confidence probabilities from a previous model\n",
    "- [x] Numerical:\n",
    "    - [x] outlier\n",
    "    - [x] out-of-bounds\n",
    "- [x] Labels: missing(Null, None, \"\", \" \"), name that number, double-struck\n",
    "- [x] Image: zeroed, null? dimensions?\n",
    "- [x] Image: add noise\n",
    "- [x] Image: flip horizonally\n",
    "- [x] Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's mess up the data a bit\n",
    "\n",
    "# Percent of the time something dirty happens (0.1%) for each method\n",
    "pct = 0.001 \n",
    "\n",
    "# Copy the splits into new dataframes to mess up\n",
    "dirty_train = raw_train.copy()\n",
    "dirty_test  = raw_test.copy()\n",
    "\n",
    "# Add a column for a hash of the images (should make it easier to compare them)\n",
    "dirty_train['image_hash'] = dirty_train['image'].apply(lambda x: sha1(x.tobytes()).hexdigest())\n",
    "dirty_test['image_hash']  =  dirty_test['image'].apply(lambda x: sha1(x.tobytes()).hexdigest())\n",
    "\n",
    "# For each row, 0.1% of the time, duplicate the row\n",
    "dirty_train = pd.concat([dirty_train, dirty_train.sample(frac=pct)])\n",
    "dirty_test  = pd.concat([dirty_test,   dirty_test.sample(frac=pct)])\n",
    "\n",
    "# For each row, 0.1% of the time, zero out the image array\n",
    "dirty_train['image'] = dirty_train['image'].apply(lambda x: np.zeros((size, size)) if np.random.rand() < pct else x)\n",
    "dirty_test['image']  =  dirty_test['image'].apply(lambda x: np.zeros((size, size)) if np.random.rand() < pct else x)\n",
    "\n",
    "# Add a column for classification scores from a previous model\n",
    "dirty_train['predict'] = np.random.normal(0.75, 0.1, dirty_train.shape[0])\n",
    "dirty_test['predict']  = np.random.normal(0.75, 0.1, dirty_test.shape[0])\n",
    "\n",
    "# For each row, 0.1% of the time, replace the predict column with a normal distribution centered on 0.25\n",
    "dirty_train['predict'] = dirty_train['predict'].apply(lambda x: np.random.normal(0.25, 0.1) if np.random.rand() < pct else x)\n",
    "dirty_test['predict']  =  dirty_test['predict'].apply(lambda x: np.random.normal(0.25, 0.1) if np.random.rand() < pct else x)\n",
    "\n",
    "# For each row, 0.1% of the time, add/subtract 1 to the predict column\n",
    "dirty_train['predict'] = dirty_train['predict'].apply(lambda x: x + 1 if np.random.rand() < pct/2 else x)\n",
    "dirty_test['predict']  =  dirty_test['predict'].apply(lambda x: x + 1 if np.random.rand() < pct/2 else x)\n",
    "dirty_train['predict'] = dirty_train['predict'].apply(lambda x: x - 1 if np.random.rand() < pct/2 else x)\n",
    "dirty_test['predict']  =  dirty_test['predict'].apply(lambda x: x - 1 if np.random.rand() < pct/2 else x)\n",
    "\n",
    "# For each row, 0.1% of the time, choose a column at random and set it to NaN\n",
    "dirty_train = dirty_train.apply(lambda row: row if np.random.rand() > pct else row.apply(lambda x: np.nan if np.random.rand() > 0.5 else x), axis=1)\n",
    "dirty_test  =  dirty_test.apply(lambda row: row if np.random.rand() > pct else row.apply(lambda x: np.nan if np.random.rand() > 0.5 else x), axis=1)\n",
    "\n",
    "# Mislabel 0.1% of the data with strings that look like labels or missing values (e.g., names like \"one\", numbers greater than 62, \" \")\n",
    "# Create a list of bad labels\n",
    "bad_labels = ['number', 'letter', 'maybe three?', 'think this is a seven', ' ', '', 'NaaN', 'Null'] + list(range(63, 100))\n",
    "# For 0.1% of the rows, randomly choose a bad label\n",
    "dirty_train['label'] = dirty_train['label'].apply(lambda x: np.random.choice(bad_labels) if np.random.rand() < pct else x)\n",
    "dirty_test['label']  =  dirty_test['label'].apply(lambda x: np.random.choice(bad_labels) if np.random.rand() < pct else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TEMPLATE: For each row, PCT% of the time, randomly apply METHOD() to the COLUMN\n",
    "# df['COLUMN'] = dirty_train['column'].apply(lambda x: METHOD(x) if np.random.rand() < PCT else x)\n",
    "\n",
    "# Not applying these for now, but they're here if you want to try them\n",
    "\n",
    "# For each row, randomly decide whether to apply a random noise\n",
    "# dirty_train['image'] = dirty_train['image'].apply(lambda x: x + np.random.rand(size, size) if np.random.rand() < 0.1 else x)\n",
    "\n",
    "# For each row, randomly decide whether to flip the image horizontally\n",
    "#dirty_train['image'] = dirty_train['image'].apply(lambda x: np.flip(x, axis=1) if np.random.rand() < 0.1 else x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "The changes above were applied randomly, so we'll need to find them and make a plan to fix them.\n",
    "\n",
    "- [x] Create a column to identify whether each row came from *train* or *test*\n",
    "- [x] (optional) Merge the data into a single\n",
    "- [x] Explore the data to understand what's in it\n",
    "- [x] List potential data issues to fix\n",
    "    - Duplicate rows\n",
    "    - Missing values\n",
    "    - Outliers and out-of-bounds issues\n",
    "    - Label issues\n",
    "    - Image issues\n",
    "    - Zeroed values\n",
    "- [x] Create a friendlier column for image labels\n",
    "- [x] Recategorizing the labels into 'numbers' and 'letters'\n",
    "- [x] Bin the prediction scores of the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start cleaning!\n",
    "\n",
    "# Labels! They're hard to understand as numbers, so let's map them to characters\n",
    "# We can do this by manually creating a dictionary:\n",
    "LABELS = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "          'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "          'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# Or generate the list of labels using the following code:\n",
    "# create the characters list, which is the digits, then uppercase, then lowercase\n",
    "chars = string.digits + string.ascii_uppercase + string.ascii_lowercase\n",
    "# create the dictionary mapping the numbers to the characters\n",
    "num_to_char = {i: chars[i] for i in range(len(chars))}\n",
    "\n",
    "# Add a column showing which split (train vs test) each row came from\n",
    "raw_train['split'] = 'train'\n",
    "raw_test['split']  = 'test'\n",
    "dirty_train['split'] = 'train'\n",
    "dirty_test['split'] = 'test'\n",
    "\n",
    "# Add a column for a hash of the images (might make it easier to compare them)\n",
    "dirty_train['image_hash'] = dirty_train['image'].apply(lambda x: sha1(np.ascontiguousarray(x)).hexdigest())\n",
    "dirty_test['image_hash']  =  dirty_test['image'].apply(lambda x: sha1(np.ascontiguousarray(x)).hexdigest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([dirty_test, dirty_train], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore merged data\n",
    "- Use `.info()` to get a general idea of shape and column types\n",
    "- Use `print()` to visualize first 5 and last 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `label` column\n",
    "Notes for cleaning: \n",
    "- Some bad labels are present\n",
    "- Solution: \n",
    "    - change the bad labels to NaN\n",
    "    - flag them in another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `image` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of one cell from the image column:\", len(merged['image'][0]))\n",
    "print(\"Sample image cell at row 0:\")\n",
    "print(merged['image'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `image_hash` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['image_hash'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of `predict` variable \n",
    "merged['predict'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the max and min values, negative values and values over 1 are probably not valid for a probability value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weird values for a predicted proportion\n",
    "print('Number of values over 1:', sum(merged['predict']>1))\n",
    "print('Number of values less than 0:', sum(merged['predict']<0))\n",
    "print('Number of values out-of-range:', sum((merged['predict']<0)|(merged['predict']>1)))\n",
    "\n",
    "# Calculating number of outliers (after deleting values out-of-range)\n",
    "mask = (0<=merged['predict']) & (merged['predict']<=1)\n",
    "desc_predict = merged[mask]['predict'].describe()\n",
    "IQR = desc_predict['75%']-desc_predict['25%']\n",
    "lower_bound = desc_predict['25%'] - 1.5 *IQR\n",
    "upper_bound = desc_predict['75%'] + 1.5 *IQR\n",
    "print('Number of outliers:', sum((merged[mask]['predict']>upper_bound) | (merged[mask]['predict']<lower_bound)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `split` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['split'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy `merged` data frame into a new data frame `work_merged` to work with\n",
    "work_merged = merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean non-sensible `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace `label` that do not make sense with NaN\n",
    "for lab in work_merged['label']: \n",
    "    if lab not in set(range(0,62)): \n",
    "        work_merged['label'].replace(lab, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace out-of-range `predict` values with `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of non-null `predict` values:', work_merged['predict'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set out-of-range condition as `predict`<0 and `predict`>1\n",
    "out_condition = (work_merged['predict']<0) | (work_merged['predict']>1)\n",
    "# Located out-of-range values and replace with NaN\n",
    "work_merged.loc[out_condition, 'predict']=np.nan\n",
    "print('Total number of non-null `predict` values after deleting out-of-range values:', work_merged['predict'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace outlier `predict` values to `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_predict = work_merged['predict'].describe()\n",
    "IQR = desc_predict['75%']-desc_predict['25%']\n",
    "lower_bound = desc_predict['25%'] - 1.5 *IQR\n",
    "upper_bound = desc_predict['75%'] + 1.5 *IQR\n",
    "outlier = (work_merged['predict']>upper_bound) | (work_merged['predict']<lower_bound)\n",
    "# Located outliers and replace with NaN\n",
    "work_merged.loc[outlier, 'predict']=np.nan\n",
    "print('Total number of non-null `predict` values after deleting outliers:', work_merged['predict'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean zero-ed out `image` arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroed = work_merged['image'].apply(lambda x: np.all(x==0))\n",
    "# Locate and replace zeroed images with NaN\n",
    "work_merged.loc[zeroed, 'image']=np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates in rows that do not contain `NaN` values across all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_null = ~work_merged.isna().any(axis=1)\n",
    "clean_merged = work_merged[Non_null]\n",
    "# Check again that `count()` for number of non-null values across columns in `work_merged[Non_null]` are the same\n",
    "clean_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unique values list as a new column in `clean_merged`\n",
    "clean_merged['unique'] = ~clean_merged.duplicated(subset=['image_hash','label','split'], keep='first')\n",
    "# Generate final cleaned dataframe `final_merged` \n",
    "final_merged = clean_merged[clean_merged['unique']==True][['label','image','image_hash', 'predict', 'split']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check info of `final_merged`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_merged.info())\n",
    "print(final_merged.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relabeling and binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a friendlier column for image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from above\n",
    "# We can do this by manually creating a dictionary:\n",
    "LABELS = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "          'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "          'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# Or generate the list of labels using the following code:\n",
    "# create the characters list, which is the digits, then uppercase, then lowercase\n",
    "chars = string.digits + string.ascii_uppercase + string.ascii_lowercase\n",
    "# create the dictionary mapping the numbers to the characters\n",
    "num_to_char = {i: chars[i] for i in range(len(chars))}\n",
    "\n",
    "# Start of solution: Create a friendlier column for image labels\n",
    "# Mapping labels and creating a `friendly_label` column\n",
    "final_merged['friendly_label'] = final_merged['label'].map(num_to_char)\n",
    "final_merged[['label','friendly_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recategorizing the labels into 'numbers' and 'letters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check string\n",
    "def check_string_type(str):\n",
    "    if str.isdigit():\n",
    "        return 'number'\n",
    "    elif str.isascii():\n",
    "        return 'letter'\n",
    "    \n",
    "#Apply function to `friendly_label` column\n",
    "final_merged['label_type']=final_merged['friendly_label'].apply(check_string_type)\n",
    "\n",
    "# Print 5 rows from each type of column \n",
    "print(final_merged[final_merged['label_type']=='number'].head(5))\n",
    "print(final_merged[final_merged['label_type']=='letter'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bin the prediction scores of the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged['predict'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin according to quadrants\n",
    "final_merged['predict_groups'] = pd.qcut(final_merged['predict'], q=4, labels =['very low', 'low', 'medium', 'high'])\n",
    "final_merged[['predict', 'predict_groups']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e515f099f0da6a2737b6685058153dc4c80cb25a1e9fe8c5e831073d214043c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
